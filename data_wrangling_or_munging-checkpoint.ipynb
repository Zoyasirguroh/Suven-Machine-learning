{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling or Data Munging\n",
    "\n",
    "Here we will concentrate on the following sub-sections of this methodology:\n",
    "\n",
    "1> Data collection: To understand different data retrieval mechanisms for \n",
    "                    different data types. -> in very brief\n",
    "\n",
    "2> Data description: To understand various attributes and properties of the\n",
    "                     data collected. -> in very brief\n",
    "\n",
    "3> Data wrangling: To prepare data for consumption in the modeling steps.\n",
    "\n",
    "4> Data visualization: To visualize different attributes for sharing results, better understanding, and so on.  -> \"Covered through matplotlib\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data Collection  -> (Covered under /pandas_for_data_analysis/data-retrieval)\n",
    "---------------\n",
    "Data is at the center of everything around us. This presents the fact that it must be present in different formats, shapes, and sizes. Its omnipresence also means that it exists in systems such as legacy machines (say mainframes), web (say web sites and web applications), databases, flat files, sensors, mobile devices, and so on.\n",
    "\n",
    "For Machine Learning we need data. Data can be collected from various sources like : CSV (Comma Separated Values), JSON (Java Script Object Notation), XML or eXtensible Markup Language, Web Scraping, SQL to query databases.\n",
    "\n",
    "Data could be structured or un-structured."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data Description\n",
    "----------------\n",
    "Data colleceted has various data types like :\n",
    "\n",
    "a> Numeric : Numeric data represents scalar information about entities being observed, for instance, number of visits to a web site, price of a product, weight of a person, and so on. For handling numeric data, we use techniques such as normalization, binning, quantization, and many more to transform numeric data as per our requirements.\n",
    "\n",
    "b> Text : Data comprising of unstructured, alphanumeric content is one of most common data types. Textual data when representing human language content contains implicit grammatical structure and meaning. This type of data requires additional care and effort for transformation and understanding.\n",
    "\n",
    "c> Categorical : This data type stands in between the numeric and text. Categorical variables refer to categories of entities being observed. For instance, hair color being black, brown, blonde and red or economic status as low, medium, or high. The values may be represented as numeric or alphanumeric, which describe properties of items in consideration. Based on certain characteristics, categorical variables can be seen as:\n",
    "\n",
    "• Nominal: These define only the category of the data point without any ordering possible. For instance, hair color can be black, brown, blonde, etc., but there cannot be any order to these categories.\n",
    "\n",
    "• Ordinal: These define category but can also be ordered based on rules on the\n",
    "context. For example, people categorized by economic status of low, medium, or\n",
    "high can be clearly ordered/sorted in the respective order."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data wrangling or data munging is the process of cleaning, transforming, and mapping data from one form to another to utilize it for tasks such as analytics, summarization, reporting, visualization, and so on.\n",
    "\n",
    "Data wrangling is one of most important and involving steps in the whole Data Science workflow. The output of this process directly impacts all downstream steps such as exploration, summarization, visualization, analysis and even the final result. This clearly shows why Data Scientists spend a lot of time in Data Collection and Wrangling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import random\n",
    "import datetime \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import warnings; warnings.simplefilter('ignore')  # to suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> The dataset in consideration has been generated using standard Python libraries like random, datetime, numpy, pandas, and so on. This dataset has been generated using a utility function called generate_sample_data().\n",
    "\n",
    ">>> Students can ignore the Utilities code and concentrate on the common data munging part, which starts as \"Generate sample data set\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_date(start,date_count):\n",
    "    \"\"\"This function generates a random date based on params\n",
    "    Args:\n",
    "        start (date object): the base date\n",
    "        date_count (int): number of dates to be generated\n",
    "    Returns:\n",
    "        list of random dates\n",
    "\n",
    "    \"\"\"\n",
    "    current = start\n",
    "    while date_count > 0:\n",
    "        curr = current + datetime.timedelta(days=randrange(42))\n",
    "        yield curr\n",
    "        date_count-=1\n",
    "\n",
    "\n",
    "def generate_sample_data(row_count=100):\n",
    "    \"\"\"This function generates a random transaction dataset\n",
    "    Args:\n",
    "        row_count (int): number of rows for the dataframe\n",
    "    Returns:\n",
    "        a pandas dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # sentinels\n",
    "    startDate = datetime.datetime(2016, 1, 1,13)\n",
    "    serial_number_sentinel = 1000\n",
    "    user_id_sentinel = 5001\n",
    "    product_id_sentinel = 101\n",
    "    price_sentinel = 2000\n",
    "    \n",
    "    \n",
    "    # base list of attributes\n",
    "    data_dict = {\n",
    "    'Serial No': np.arange(row_count)+serial_number_sentinel,\n",
    "    'Date': np.random.permutation(pd.to_datetime([x.strftime(\"%d-%m-%Y\") \n",
    "                                                    for x in _random_date(startDate,\n",
    "                                                                          row_count)]).date\n",
    "                                  ),\n",
    "    'User ID': np.random.permutation(np.random.randint(0,\n",
    "                                                       row_count,\n",
    "                                                       size=int(row_count/10)) + user_id_sentinel).tolist()*10,\n",
    "    'Product ID': np.random.permutation(np.random.randint(0,\n",
    "                                                          row_count,\n",
    "                                                          size=int(row_count/10))+ product_id_sentinel).tolist()*10 ,\n",
    "    'Quantity Purchased': np.random.permutation(np.random.randint(1,\n",
    "                                                                  42,\n",
    "                                                                  size=row_count)),\n",
    "    'Price': np.round(np.abs(np.random.randn(row_count)+1)*price_sentinel,\n",
    "                      decimals=2),\n",
    "    'User Type':np.random.permutation([chr(random.randrange(97, 97 + 3 + 1)) \n",
    "                                            for i in range(row_count)])\n",
    "    }\n",
    "    \n",
    "    # introduce missing values\n",
    "    for index in range(int(np.sqrt(row_count))): \n",
    "        data_dict['Price'][np.argmax(data_dict['Price'] == random.choice(data_dict['Price']))] = np.nan\n",
    "        data_dict['User Type'][np.argmax(data_dict['User Type'] == random.choice(data_dict['User Type']))] = np.nan\n",
    "        data_dict['Date'][np.argmax(data_dict['Date'] == random.choice(data_dict['Date']))] = np.nan\n",
    "        data_dict['Product ID'][np.argmax(data_dict['Product ID'] == random.choice(data_dict['Product ID']))] = 0\n",
    "        data_dict['Serial No'][np.argmax(data_dict['Serial No'] == random.choice(data_dict['Serial No']))] = -1\n",
    "        data_dict['User ID'][np.argmax(data_dict['User ID'] == random.choice(data_dict['User ID']))] = -101\n",
    "        \n",
    "    \n",
    "    # create data frame\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def describe_dataframe(df=pd.DataFrame()):\n",
    "    \"\"\"This function generates descriptive stats of a dataframe\n",
    "    Args:\n",
    "        df (dataframe): the dataframe to be analyzed\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\"*30)\n",
    "    print(\"About the Data\")\n",
    "    print(\"*\"*30)\n",
    "    \n",
    "    print(\"Number of rows::\",df.shape[0])\n",
    "    print(\"Number of columns::\",df.shape[1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Column Names::\",df.columns.values.tolist())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Column Data Types::\\n\",df.dtypes)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Columns with Missing Values::\",df.columns[df.isnull().any()].tolist())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Number of rows with Missing Values::\",len(pd.isnull(df).any(1).nonzero()[0].tolist()))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Sample Indices with missing data::\",pd.isnull(df).any(1).nonzero()[0].tolist()[0:5])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"General Stats::\")\n",
    "    print(df.info())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Summary Stats::\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Dataframe Sample Rows::\")\n",
    "    display(df.head(5))\n",
    "    \n",
    "def cleanup_column_names(df,rename_dict={},do_inplace=True):\n",
    "    \"\"\"This function renames columns of a pandas dataframe\n",
    "       It converts column names to snake case if rename_dict is not passed. \n",
    "    Args:\n",
    "        rename_dict (dict): keys represent old column names and values point to \n",
    "                            newer ones\n",
    "        do_inplace (bool): flag to update existing dataframe or return a new one\n",
    "    Returns:\n",
    "        pandas dataframe if do_inplace is set to False, None otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    if not rename_dict:\n",
    "        return df.rename(columns={col: col.lower().replace(' ','_') \n",
    "                    for col in df.columns.values.tolist()}, \n",
    "                  inplace=do_inplace)\n",
    "    else:\n",
    "        return df.rename(columns=rename_dict,inplace=do_inplace)\n",
    "\n",
    "def expand_user_type(u_type):\n",
    "    \"\"\"This function maps user types to user classes\n",
    "    Args:\n",
    "        u_type (str): user type value\n",
    "    Returns:\n",
    "        (str) user_class value\n",
    "\n",
    "    \"\"\"\n",
    "    if u_type in ['a','b']:\n",
    "        return 'new'\n",
    "    elif u_type == 'c':\n",
    "        return 'existing'\n",
    "    elif u_type == 'd':\n",
    "        return 'loyal_existing'\n",
    "    else:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_sample_data(row_count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "******************************\n",
      "About the Data\n",
      "******************************\n",
      "Number of rows:: 1000\n",
      "Number of columns:: 7\n",
      "\n",
      "\n",
      "Column Names:: ['Serial No', 'Date', 'User ID', 'Product ID', 'Quantity Purchased', 'Price', 'User Type']\n",
      "\n",
      "\n",
      "Column Data Types::\n",
      " Serial No               int32\n",
      "Date                   object\n",
      "User ID                 int64\n",
      "Product ID              int64\n",
      "Quantity Purchased      int32\n",
      "Price                 float64\n",
      "User Type              object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Columns with Missing Values:: ['Date', 'Price']\n",
      "\n",
      "\n",
      "Number of rows with Missing Values:: 61\n",
      "\n",
      "\n",
      "Sample Indices with missing data:: [3, 4, 5, 9, 15]\n",
      "\n",
      "\n",
      "General Stats::\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      "Serial No             1000 non-null int32\n",
      "Date                  969 non-null object\n",
      "User ID               1000 non-null int64\n",
      "Product ID            1000 non-null int64\n",
      "Quantity Purchased    1000 non-null int32\n",
      "Price                 969 non-null float64\n",
      "User Type             1000 non-null object\n",
      "dtypes: float64(1), int32(2), int64(2), object(2)\n",
      "memory usage: 39.1+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Summary Stats::\n",
      "         Serial No      User ID   Product ID  Quantity Purchased        Price\n",
      "count  1000.000000  1000.000000  1000.000000         1000.000000   969.000000\n",
      "mean   1451.053000  5525.524000   591.316000           21.219000  2311.280609\n",
      "std     384.070676   349.662731   279.436338           11.958597  1623.155035\n",
      "min      -1.000000  -101.000000     0.000000            1.000000     0.970000\n",
      "25%    1224.750000  5279.000000   356.000000           11.000000   962.280000\n",
      "50%    1480.500000  5534.000000   578.000000           21.500000  2056.780000\n",
      "75%    1737.250000  5796.250000   808.500000           32.000000  3371.320000\n",
      "max    1999.000000  5998.000000  1083.000000           41.000000  8481.130000\n",
      "\n",
      "\n",
      "Dataframe Sample Rows::\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>Date</th>\n",
       "      <th>User ID</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Quantity Purchased</th>\n",
       "      <th>Price</th>\n",
       "      <th>User Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>-101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1873.31</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>5982</td>\n",
       "      <td>692</td>\n",
       "      <td>30</td>\n",
       "      <td>6377.70</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5412</td>\n",
       "      <td>268</td>\n",
       "      <td>18</td>\n",
       "      <td>1746.93</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5970</td>\n",
       "      <td>1083</td>\n",
       "      <td>26</td>\n",
       "      <td>1276.04</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5961</td>\n",
       "      <td>212</td>\n",
       "      <td>2</td>\n",
       "      <td>1526.24</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No        Date  User ID  Product ID  Quantity Purchased    Price  \\\n",
       "0       1000  2016-10-01     -101           0                   2  1873.31   \n",
       "1       1001  2016-08-02     5982         692                  30  6377.70   \n",
       "2       1002  2016-10-01     5412         268                  18  1746.93   \n",
       "3       1003         NaN     5970        1083                  26  1276.04   \n",
       "4       1004         NaN     5961         212                   2  1526.24   \n",
       "\n",
       "  User Type  \n",
       "0         n  \n",
       "1         n  \n",
       "2         n  \n",
       "3         n  \n",
       "4         n  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "describe_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns:\n",
      "['Serial No', 'Date', 'User ID', 'Product ID', 'Quantity Purchased', 'Price', 'User Type']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe columns:\\n{}\".format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns:\n",
      "['serial_no', 'date', 'user_id', 'product_id', 'quantity_purchased', 'price', 'user_type']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe columns:\\n{}\".format(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort Rows on defined attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# .. type your code here\n",
    "\n",
    "\n",
    "\n",
    "# first sorted on serial_no, all products having same serial_no sorted on price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange Columns in a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. type your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Column Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10 values from column at index 3\n",
    "print(df.iloc[:,3].values[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10 values of quantity purchased\n",
    "print(df.quantity_purchased.values[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Column Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10 values of columns with data type float\n",
    "print(df.select_dtypes(include=['float64']).values[:10,0])\n",
    "# we are printing only 0th column, i.e price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select specific rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.iloc[[10,501,20]]) # i -> implicit indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude Specific Row indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.drop([0,2,5], axis=0).head())\n",
    "# note : axis=1 would give error as, indexes 0,2,5 don't appear column wise\n",
    "# default axis is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df.quantity_purchased>25].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offset from top of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[100:].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offset from bottom of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[-10:].head()) # 1000th row is the last row - 10 = 990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TypeCasting/Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date)\n",
    "# compare dtypes of the original df with this one\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply/Map Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map : Create a derived attribute using map. map() works element wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_class'] = df['user_type'].map(expand_user_type)\n",
    "# map function applies the user defn method expand_user_type to each value of \n",
    "# user_type\n",
    "display(df.tail())\n",
    "\n",
    "# -- expand_user_type function is defn in the Utilities. \n",
    "# -- Its body is shown here just for reference.\n",
    "#  def expand_user_type(u_type):\n",
    "#   if u_type in ['a','b']:\n",
    "#   return 'new'\n",
    "#   elif u_type == 'c':\n",
    "#   return 'existing'\n",
    "#   elif u_type == 'd':\n",
    "#   return 'loyal_existing'\n",
    "#   else:\n",
    "#   return 'error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply: Using apply to get attribute ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The apply() function is used to perform actions on the whole object, \n",
    "# depending upon the axis (default is on all rows).\n",
    "display(df.select_dtypes(include=[np.number]).apply(lambda x: \n",
    "                                                        x.max()- x.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applymap: Extract week from date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purchase_week'] = df[['date']].applymap(lambda dt:dt.week \n",
    "                                                if not pd.isnull(dt.week) \n",
    "                                                else 0)\n",
    "\n",
    "# lambda fn gets the week of the transaction from the date attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " display(df.head()) # display() is defined under Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing Missing Values : Missing values can lead to all sorts of problems when dealing with Machine Learning and Data Science related use cases. Not only can they cause problems for algorithms, they can mess up calculations and even final outcomes. \n",
    "\n",
    "Missing values also pose risk of being interpreted in non-standard ways as well leading to confusion and more errors. Hence, imputing missing values carries a lot of weight in the overall data wrangling process.\n",
    "\n",
    "One of the easiest ways of handling missing values is to ignore or remove them altogether from the dataset. When the dataset is fairly large and we have enough samples of various types required, this option can be safely exercised. We use the dropna() function from pandas in the following snippet to remove rows of data where the date of transaction is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Drop Rows with missing dates::\" )\n",
    "df_dropped = df.dropna(subset=['date'])\n",
    "display(df_dropped.head())\n",
    "\n",
    "# # dropna -> drops not available values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing Price values with mean price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often dropping rows is a very expensive and unfeasible option. \n",
    "# In many scenarios, missing values are imputed using the help of other \n",
    "# values in the dataframe. One commonly used trick is to replace missing\n",
    "# values with a central tendency measure like mean or median.\n",
    "# fillna -> fills not available values\n",
    "df_dropped['price'].fillna(value=np.round(df.price.mean(),decimals=2),\n",
    "                                inplace=True)\n",
    "df_dropped['price'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing user_type values with value from previous row (forward fill) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fill Missing user_type values with value from previous row (forward fill) ::\" )\n",
    "df_dropped['user_type'].fillna(method='ffill',inplace=True)\n",
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Missing user_type values with value from next row (backward fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['user_type'].fillna(method='bfill',inplace=True)\n",
    "# keeping inplace true writes the new data in the data set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicate serial_no rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample duplicates\n",
    "# duplicated is a build-in fn\n",
    "display(df_dropped[df_dropped.duplicated(subset=['serial_no'])].head())\n",
    "print(\"Shape of df={}\".format(df_dropped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_duplicates is a build-in function \n",
    "df_dropped.drop_duplicates(subset=['serial_no'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated dataframe\n",
    "display(df_dropped.head())\n",
    "print(\"Shape of df={}\".format(df_dropped.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows which have less than 3 attributes with non-missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are certain conditions where a record is not much of use \n",
    "# if it has more than a certain threshold of attribute values missing. \n",
    "# For instance, if in our dataset a transaction has less than three\n",
    "# attributes as non-null, the transaction might almost be unusable. \n",
    "# In such a scenario, it might be advisable to drop that data point itself. \n",
    "# We can filter out such data points using the function dropna() \n",
    "# with the parameter thresh set to the threshold of non-null attributes\n",
    "\n",
    "display(df.dropna(thresh=3).head())\n",
    "print(\"Shape of df={}\".format(df.dropna(thresh=3).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding using get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to convert the categorical variable into indicator variables \n",
    "# use the get_dummies() function.\n",
    "display(pd.get_dummies(df,columns=['user_type']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the map() function, where we simply map each value \n",
    "# from the allowed set to a numeric value\n",
    "type_map={'a':0,'b':1,'c':2,'d':3,np.NAN:-1}\n",
    "df['encoded_user_type'] = df.user_type.map(type_map)\n",
    "display((df.tail()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling data from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.sample(frac=0.2, replace=True, random_state=42).head())\n",
    "# explaination for replace parameter\n",
    "# when sampling the records are removed from the orginal dataset, so that\n",
    "# their is no repetition of samples in the o/p.\n",
    "# but if the size of the sample is greater than the dataset itself then keep\n",
    "# replace = true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Numeric Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute normalization is the process of standardizing the range of values of attributes. Machine learning algorithms in many cases utilize distance metrics, attributes or features of different scales/ranges which might adversely affect the calculations or bias the outcomes. Normalization is also called feature scaling.\n",
    "\n",
    "Normalize price values using  **Min-Max Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df.dropna().copy()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df_normalized['price'].values.reshape(-1,1))\n",
    "df_normalized['price'] = np_scaled.reshape(-1,1)\n",
    "\n",
    "# reshape(-1,1) works like this :\n",
    "# -1 means we dont know the no. of rows. hence it would take len(df).\n",
    "# the 2nd parameter being 1 -> means I want only 1 column\n",
    "# ex: assume z is 2D numpy array and z.shape is (3,4) \n",
    "# z.reshape(-1) would give you a 1D np.array\n",
    "# now z.shape would be (12,)  like \n",
    "# array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "# and reshaping it as z.reshape(-1,1) would give us (12,1). \n",
    "# i.e 1 column with all row values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize quantity purchased values using  **Robust Scaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = df.dropna().copy()\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "rs_scaled = robust_scaler.fit_transform(df_normalized['quantity_purchased'].values.reshape(-1,1))\n",
    "df_normalized['quantity_purchased'] = rs_scaled.reshape(-1,1)\n",
    "\n",
    "# interested people may research the maths behind min-max scaler and\n",
    "# robust_scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data summarization refers to the process of preparing a compact representation of raw data at hand. This process involves aggregation of data using different statistical, mathematical, and other methods. Summarization is helpful for visualization, compressing raw data, and better understanding of its attributes.\n",
    "\n",
    "Condition based aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean price of items purchased by user_type=a :: {}\".format(df['price'][df['user_type']=='a'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condtion based counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['purchase_week'].value_counts())\n",
    "# counts the number of transactions per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group By certain attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(['user_class'])['quantity_purchased'].sum())\n",
    "# This statement generates a tabular output representing \n",
    "# sum of quantities purchased by each user_class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group By with different aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The groupby() function is a powerful interface that allows us \n",
    "# to perform complex groupings and aggregations.\n",
    "# With groupby() we can perform multi-attribute groupings \n",
    "# and apply multiple aggregations across attributes.\n",
    "\n",
    "# variant-1: multiple aggregations on single attribute\n",
    "display(df.groupby(['user_class'])['quantity_purchased'].agg([np.sum,\n",
    "                                                                np.mean,\n",
    "                                                                np.count_nonzero]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by specific aggregate functions for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant-2: different aggregation functions for each attribute\n",
    "display(df.groupby(['user_class','user_type']).agg({'price':np.mean,\n",
    "                                                        'quantity_purchased':np.max}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by with multiple agg for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 3: Here, we do a combination of variants 1 and 2, \n",
    "# i.e., we apply multiple aggregations on the price field while \n",
    "# applying only a single one on quantity_purchased. \n",
    "# Note : a dictionary is passed, as shown in the snippet.\n",
    "display(df.groupby(['user_class','user_type']).agg({'price':{  'total_price':np.sum,\n",
    "                                                                'mean_price':np.mean,\n",
    "                                                                'variance_price':np.std,\n",
    "                                                                'count':np.count_nonzero},\n",
    "                                                   'quantity_purchased':np.sum}))  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recommeded work :\n",
    "-----------------\n",
    "Apart from groupby() based summarization, other functions such as pivot(), pivot_table(), stack(), unstack(), crosstab(), and melt() provide capabilities to reshape the pandas dataframe as per requirements. \n",
    "\n",
    "A complete description of these methods with examples is available as part of pandas documentation at \n",
    "https://pandas.pydata.org/pandas-docs/stable/reshaping.html. \n",
    "I recommend you to go through the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.groupby(['user_class','user_type']).agg({'price':{  'total_price':np.sum,\n",
    "                                                                'mean_price':np.mean,\n",
    "                                                                'variance_price':np.std,\n",
    "                                                                'count':np.count_nonzero},\n",
    "                                                   'quantity_purchased':np.sum}))  \n",
    "\n",
    "# pivot table shows us comprehensive information of mean price \n",
    "# date-wise , user_type wise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

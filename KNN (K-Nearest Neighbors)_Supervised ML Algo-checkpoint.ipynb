{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Algorithm using Scikit-Learn\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. \n",
    "\n",
    "> It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. \n",
    "\n",
    "KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory\n",
    "----------\n",
    "The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong.\n",
    "\n",
    "Let's see this algorithm in action with the help of a simple example. Suppose you have a dataset with two variables, which when plotted, looks like the one in the following figure.\n",
    "\n",
    "> Manhattan distance between 2 points is the distance along the axis, like you measure the distance in a right angled way. Usually used in circuits to measure the distance wires take; as the wires cannot run in between the circuit-boards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample_data_points_for_KNN](images/sample_data_points_for_KNN.png 'sample_data_points_for_KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to classify a new data point with 'X' into \"Blue\" class or \"Red\" class. The coordinate values of the data point are x=45 and y=50. Suppose the value of K is 3. The KNN algorithm starts by calculating the distance of point X from all the points. It then finds the 3 nearest points with least distance to point X. This is shown in the figure below. The three nearest points have been encircled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample_data_points_for_KNN](images/sample_2_data_points_for_KNN.png 'sample_2_data_points_for_KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of the KNN algorithm is to assign new point to the class to which majority of the three nearest points belong. From the figure above we can see that the two of the three nearest points belong to the class \"Red\" while one belongs to the class \"Blue\". Therefore the new data point will be classified as \"Red\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing KNN Algorithm with Scikit-Learn\n",
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with ... load the iris data set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data  -> drop species to get X, only extract species to get Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train Test Split  -> use train_test_split()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "# Before making any actual predictions, it is always \n",
    "# a good practice to scale the features so that all of them \n",
    "# can be uniformly evaluated.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "# make StandardScaler object\n",
    "\n",
    "# fit scalar-object over the X_train dataset\n",
    "\n",
    "\n",
    "# use scalar-object to transform the X_train and X_test data set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training and Predictions\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that our KNN algorithm was able to classify all the 29 out of 30 records in the test set with 97% accuracy, which is excellent. Although the algorithm performed very well with this dataset, don't expect the same results with all applications. As noted earlier, KNN doesn't always perform as well with high-dimensionality or categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Error Rate with the K Value\n",
    "-------------------------------------------------------\n",
    "In the training and prediction section we said that there is no way to know beforehand which value of K that yields the best results in the first go. We randomly chose 5 as the K value and it just happen to result in 100% accuracy.\n",
    "\n",
    "One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.\n",
    "\n",
    "In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 40.\n",
    "\n",
    "To do so, let's first calculate the mean of error for all the predicted values where K ranges from 1 and 40. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 40):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test)) #here != compares 2 ndarray objects \n",
    "    # return value is array of true/false i.e 0/1 values , of which we are \n",
    "    # finding the mean values.\n",
    "    \n",
    "    \n",
    "    # The above script executes a loop from 1 to 40. \n",
    "    # In each iteration the mean error for predicted values of test set \n",
    "    # is calculated and the result is appended to the error list.\n",
    "    \n",
    "# The next step is to plot the error values against K values.\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')  \n",
    "plt.xlabel('K Value')  \n",
    "plt.ylabel('Mean Error')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the output we can see that the mean error is ~0.03 when the value of the K is between 11 and 27. This o/p would differ for every participant/student of my course, as the train-test sets formed differ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros\n",
    "------\n",
    "1> It is extremely easy to implement\n",
    "\n",
    "2> It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.\n",
    "Since the algorithm requires no training before making predictions, new data can be added seamlessly.\n",
    "\n",
    "3> There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "\n",
    "Cons\n",
    "-------\n",
    "1> The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.\n",
    "\n",
    "2> The KNN algorithm has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.\n",
    "\n",
    "3> Finally, the KNN algorithm doesn't work well with categorical features since it is difficult to find the distance between dimensions with categorical features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
